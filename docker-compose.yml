services:
  ollama:
    container_name: Ollama
    networks:
      - ollamallms
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - LLMS:/root/.ollama
      
      # - ./containers/ollama:/app
    # restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  anythingllm:
    image: mintplexlabs/anythingllm:latest
    ports:
      - "3001:3001"
    volumes:
      - anythingllm-data:/app/server/storage
    depends_on:
      - ollama
    restart: unless-stopped
    networks:
      - ollamallms
    environment:
      - STORAGE_DIR=/app/server/storage
      - OLLAMA_BASE_URL=http://ollama:11434

    extra_hosts:
      - "host.docker.internal:host-gateway"
  model_selection:
    build:
      context: ./containers/model_selection
    ports:
      - 5100:5100
    networks:
      - ollamallms


networks:
  ollamallms:
    driver: bridge

volumes:
  LLMS:
    name: ollamaLLM-storage
  anythingllm-data:
    name: anythingllm-data
      